{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSAYDuXxepmL",
        "outputId": "fa376673-c393-4f91-905b-51a1f3e9717e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.02 s, sys: 29.9 ms, total: 6.05 s\n",
            "Wall time: 6.18 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['uid', 'session_id', 'user_id', 'timestamp', 'event_type', 'screen_x',\n",
              "       'screen_y', 'user_id_new', 'session_id_new', 'e',\n",
              "       ...\n",
              "       'mean_second_xy_speed_shifted_29', 'mean_second_screen_x_shifted_29',\n",
              "       'mean_second_screen_y_shifted_29', 'mean_second_event_type_shifted_29',\n",
              "       'mean_second_x_speed_shifted_30', 'mean_second_y_speed_shifted_30',\n",
              "       'mean_second_xy_speed_shifted_30', 'mean_second_screen_x_shifted_30',\n",
              "       'mean_second_screen_y_shifted_30', 'mean_second_event_type_shifted_30'],\n",
              "      dtype='object', length=215)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from enum import Enum\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "import random\n",
        "import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "     \n",
        "\n",
        "# Prepare Data functions\n",
        "# get mean features per second\n",
        "def get_mean_features_sec(df, mean_features, groupby_features):\n",
        "  for column_name in mean_features:\n",
        "    new_column_name = 'mean_second_' + column_name\n",
        "    df[new_column_name] = df.groupby(groupby_features)[column_name].transform('mean')\n",
        "  return df\n",
        "\n",
        "# get mean features per mintue\n",
        "def get_mean_features_minute(df, mean_features, groupby_features):\n",
        "  for column_name in mean_features:\n",
        "    new_column_name = 'mean_minute_' + column_name\n",
        "    df[new_column_name] = df.groupby(groupby_features)[column_name].transform('mean')\n",
        "  return df\n",
        "\n",
        "\n",
        "# add shift features \n",
        "def get_shift(n, columns, df, session_column_name):\n",
        "  features_ = []\n",
        "  df_data = df\n",
        "  for i in range(n):\n",
        "    for c in columns:\n",
        "      new_feature_name = c + '_shifted_' + str(i+1)\n",
        "      df_data[new_feature_name] = df_data.groupby(session_column_name)[c].shift(i+1)\n",
        "      features_ = features_ + [new_feature_name]\n",
        "  return df_data, features_\n",
        "\n",
        "df_data = pd.read_csv('./Train_Mouse.csv')\n",
        "df_data.shape\n",
        "\n",
        "# sorted buy timestamp\n",
        "df_data = df_data.sort_values('timestamp').reset_index().drop(labels='index', axis=1)\n",
        "\n",
        "# labeling user_id and session_id\n",
        "user_id_array = df_data['user_id'].unique()\n",
        "user_id_map = {val:idx for idx,val in enumerate(user_id_array)}\n",
        "df_data['user_id_new'] = df_data['user_id'].map(lambda x: user_id_map[x])\n",
        "session_id_array = df_data['session_id'].unique()\n",
        "session_id_map = {val:idx for idx,val in enumerate(session_id_array)}\n",
        "df_data['session_id_new'] = df_data['session_id'].map(lambda x: session_id_map[x])\n",
        "\n",
        "# labeling + one-hot for event_type\n",
        "event_list = ['', 'RELEASE', 'MOVE', 'WHEEL', 'DRAG', 'CLICK']\n",
        "df_data['e'] = df_data['event_type'].map(lambda x: event_list[x])\n",
        "one_hot_df = pd.get_dummies(df_data['e'], prefix='event')\n",
        "df_data = pd.concat([df_data, one_hot_df], axis=1)\n",
        "categorial_cols = ['event_MOVE', 'event_WHEEL', 'event_DRAG', 'event_CLICK']\n",
        "\n",
        "# time features\n",
        "df_data['datetime'] = pd.to_datetime(df_data['timestamp'], unit='ms')\n",
        "df_data['day'] = df_data['datetime'].dt.date\n",
        "df_data['hour'] = df_data['datetime'].dt.hour\n",
        "df_data['minute'] = df_data['datetime'].dt.minute\n",
        "df_data['second'] = df_data['datetime'].dt.second\n",
        "time_features = ['hour', 'minute', 'second']\n",
        "\n",
        "# timestamp features\n",
        "df_data['time_stamp_min'] = df_data.groupby('session_id_new')['timestamp'].transform('min')\n",
        "df_data['time_stamp'] = df_data['timestamp'] - df_data['time_stamp_min']\n",
        "df_data['time_diff'] = df_data.groupby('session_id_new')['timestamp'].diff()\n",
        "\n",
        "# calculate the x_diff y_diff and distance btw (x,y)\n",
        "df_data['x_diff'] = df_data.groupby('session_id_new')['screen_x'].diff()\n",
        "df_data['y_diff'] = df_data.groupby('session_id_new')['screen_y'].diff()\n",
        "df_data['xy_diff'] = np.sqrt(df_data['x_diff']**2 + df_data['y_diff']**2)\n",
        "\n",
        "# calculate the speed\n",
        "df_data['x_speed'] = np.abs(df_data['x_diff'] / df_data['time_diff'])\n",
        "df_data['y_speed'] = np.abs(df_data['y_diff'] / df_data['time_diff'])\n",
        "df_data['xy_speed'] = df_data['xy_diff'] / df_data['time_diff']\n",
        "\n",
        "df_data = df_data.dropna()\n",
        "\n",
        "\n",
        "\n",
        "# Anormaly Detection\n",
        "model = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(0.05), random_state=42)\n",
        "anomaly_features = ['screen_x', 'screen_y', 'x_speed', 'y_speed', 'xy_speed']\n",
        "anomaly_mask_pd = pd.DataFrame()\n",
        "\n",
        "for column_name in anomaly_features:\n",
        "  model.fit(df_data[column_name].values.reshape(-1, 1))\n",
        "  # Make predictions on the data\n",
        "  anomaly_mask = model.predict(df_data[column_name].values.reshape(-1, 1)) == -1\n",
        "  index_new_name = 'ab_'+column_name\n",
        "  anomaly_mask_pd[index_new_name] = anomaly_mask\n",
        "\n",
        "anomaly_mask_pd['pick_index'] = ~(anomaly_mask_pd.iloc[:, 0:5].any(axis=1))\n",
        "\n",
        "df_data = df_data[anomaly_mask_pd['pick_index'].values]\n",
        "df_data = df_data.reset_index().drop(labels='index', axis=1)\n",
        "\n",
        "# Feature Engineering\n",
        "mean_features = ['x_speed', 'y_speed', 'xy_speed', 'screen_x', 'screen_y', 'event_type']\n",
        "df_data = get_mean_features_sec(df_data.dropna(), mean_features, ['session_id_new', 'day', 'hour', 'minute', 'second'])\n",
        "\n",
        "# we take 30 step\n",
        "aggragation_cols = [ 'mean_second_x_speed', 'mean_second_y_speed', 'mean_second_xy_speed', \n",
        "                     'mean_second_screen_x', 'mean_second_screen_y', 'mean_second_event_type'\n",
        "                  ]\n",
        "n_step = 30\n",
        "df_data, features_add = get_shift(n_step, aggragation_cols, df_data, 'session_id_new')\n",
        "\n",
        "df_data.columns\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aqfv4AAImHZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Split"
      ],
      "metadata": {
        "id": "66uD60KrfTNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get test dataset: each user one session id\n",
        "def get_valid_idx(df_data_, n, random_seed = False):\n",
        "  if random_seed:\n",
        "    random.seed()\n",
        "  idx_valid = []\n",
        "  for i in df_data_.groupby('user_id_new')['session_id_new'].unique():\n",
        "    idx_valid.append(random.sample(list(i), 2)[0])\n",
        "    idx_valid.append(random.sample(list(i), 2)[1])\n",
        "  return np.sort(idx_valid)\n",
        "\n",
        "idx_valid = get_valid_idx(df_data, 5)\n",
        "print(f\"The session_id picked as test dataset: \\n {idx_valid}\")\n",
        "\n",
        "df_valid = df_data[df_data['session_id_new'].isin(idx_valid)]; \n",
        "df_train = df_data[~df_data.index.isin(df_valid.index)]; \n",
        "df_valid = df_valid.dropna().reset_index().drop(labels='index', axis=1)\n",
        "df_train = df_train.dropna().reset_index().drop(labels='index', axis=1)\n",
        "\n",
        "features_for_train =  features_add + aggragation_cols # numerical_cols + categorial_cols\n",
        "\n",
        "# get the label as well, check the data shape \n",
        "df_train_x = df_train[features_for_train]; df_train_y = df_train['user_id_new'] \n",
        "df_valid_x = df_valid[features_for_train];  df_valid_y = df_valid['user_id_new'] \n",
        "# when need test dataset: \n",
        "# df_test_x = df_test[features_for_train];  df_test_y = df_test['user_id_new'] \n",
        "\n",
        "df_train_x.shape, df_valid_x.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbZtAS4hfUmb",
        "outputId": "04c21909-9cfa-4d20-9985-fef69df64095"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The session_id picked as test dataset: \n",
            " [  1   4   9  11  11  12  27  30  33  34  35  36  41  50  53  55  56  61\n",
            "  66  66  76  79  80  83  86  89  91  95  96  99 103 105 107 108 111 112\n",
            " 113 116 117 119]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4226, 186), (1972, 186))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (0,1) Normalization \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "# fit the scaler to the training data and transform the training data\n",
        "X_train_scaled = scaler.fit_transform(df_train_x)\n",
        "\n",
        "# transform the testing data using the scaler fitted on the training data\n",
        "X_valid_scaled = scaler.transform(df_valid_x)\n"
      ],
      "metadata": {
        "id": "Y6CvdBoYmOR_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LR"
      ],
      "metadata": {
        "id": "FCEnOGK_fbNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# create a Logistic Regression model\n",
        "model = LogisticRegression(multi_class='ovr', C=10)\n",
        "\n",
        "# train the model on the training set\n",
        "model.fit(X_train_scaled, df_train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwBqSSvDfb6i",
        "outputId": "8848f90f-c621-4c60-b92c-fa3dc4441c35"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.81 s, sys: 2.09 s, total: 6.91 s\n",
            "Wall time: 5.6 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, multi_class='ovr')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "9FnG0oeomwuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_train = model.predict(X_train_scaled)\n",
        "print(f\"confusion matrix on train dataset: \\n {classification_report(df_train_y.values, y_pred_train)}\")\n",
        "\n",
        "y_pred_valid = model.predict(X_valid_scaled)\n",
        "print(f\"confusion matrix on test dataset: \\n {classification_report(df_valid_y.values, y_pred_valid)}\")\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZNnLIuikzXj",
        "outputId": "bebdb776-0baa-4e0e-e51d-d573bcd620ea"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix on train dataset: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.58      0.63       126\n",
            "           1       0.39      0.35      0.37       158\n",
            "           2       0.53      0.65      0.59       268\n",
            "           3       0.43      0.34      0.38       255\n",
            "           4       0.79      0.91      0.85       225\n",
            "           5       0.72      0.75      0.74       247\n",
            "           6       0.36      0.38      0.36       349\n",
            "           7       0.41      0.21      0.28       291\n",
            "           8       0.58      0.83      0.68       356\n",
            "           9       0.14      0.06      0.09       277\n",
            "          10       0.53      0.57      0.55       118\n",
            "          11       0.61      0.28      0.39       128\n",
            "          12       0.96      0.98      0.97       116\n",
            "          13       0.65      0.86      0.74       187\n",
            "          14       0.59      0.57      0.58       106\n",
            "          15       0.62      0.70      0.66       376\n",
            "          16       1.00      1.00      1.00       166\n",
            "          17       0.69      0.83      0.76       106\n",
            "          18       0.42      0.43      0.42       187\n",
            "          19       0.71      0.83      0.76       184\n",
            "\n",
            "    accuracy                           0.58      4226\n",
            "   macro avg       0.59      0.61      0.59      4226\n",
            "weighted avg       0.56      0.58      0.56      4226\n",
            "\n",
            "confusion matrix on test dataset: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.55      0.47        44\n",
            "           1       0.26      0.26      0.26        73\n",
            "           2       0.00      0.00      0.00        81\n",
            "           3       0.60      0.43      0.50       165\n",
            "           4       0.26      0.28      0.27       151\n",
            "           5       0.43      0.56      0.49       108\n",
            "           6       0.23      0.32      0.26       129\n",
            "           7       0.19      0.18      0.18       111\n",
            "           8       0.40      0.80      0.53        74\n",
            "           9       0.06      0.15      0.09        26\n",
            "          10       0.32      0.15      0.21        98\n",
            "          11       0.03      0.01      0.02        87\n",
            "          12       0.60      0.59      0.59       124\n",
            "          13       0.61      0.29      0.39       104\n",
            "          14       0.42      0.16      0.23       106\n",
            "          15       0.45      0.62      0.52       143\n",
            "          16       1.00      0.84      0.91       125\n",
            "          17       0.03      0.02      0.02        56\n",
            "          18       0.39      0.21      0.28       117\n",
            "          19       0.32      0.58      0.41        50\n",
            "\n",
            "    accuracy                           0.37      1972\n",
            "   macro avg       0.35      0.35      0.33      1972\n",
            "weighted avg       0.39      0.37      0.36      1972\n",
            "\n"
          ]
        }
      ]
    }
  ]
}