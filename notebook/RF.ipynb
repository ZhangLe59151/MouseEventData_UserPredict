{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ELTKcLJf4pt7"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from enum import Enum\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import random\n",
        "import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing & Feature Engineering"
      ],
      "metadata": {
        "id": "PgufvqXzBUEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data functions\n",
        "# get mean features per second\n",
        "def get_mean_features_sec(df, mean_features, groupby_features):\n",
        "  for column_name in mean_features:\n",
        "    new_column_name = 'mean_second_' + column_name\n",
        "    df[new_column_name] = df.groupby(groupby_features)[column_name].transform('mean')\n",
        "  return df\n",
        "\n",
        "# get mean features per mintue\n",
        "def get_mean_features_minute(df, mean_features, groupby_features):\n",
        "  for column_name in mean_features:\n",
        "    new_column_name = 'mean_minute_' + column_name\n",
        "    df[new_column_name] = df.groupby(groupby_features)[column_name].transform('mean')\n",
        "  return df\n",
        "\n",
        "\n",
        "# add shift features \n",
        "def get_shift(n, columns, df, session_column_name):\n",
        "  features_ = []\n",
        "  df_data = df\n",
        "  for i in range(n):\n",
        "    for c in columns:\n",
        "      new_feature_name = c + '_shifted_' + str(i+1)\n",
        "      df_data[new_feature_name] = df_data.groupby(session_column_name)[c].shift(i+1)\n",
        "      features_ = features_ + [new_feature_name]\n",
        "  return df_data, features_"
      ],
      "metadata": {
        "id": "n0FjowAB-m4m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df_data = pd.read_csv('./Train_Mouse.csv')\n",
        "df_data.shape\n",
        "\n",
        "# sorted buy timestamp\n",
        "df_data = df_data.sort_values('timestamp').reset_index()\n",
        "df_data['index'] = df_data.index\n",
        "\n",
        "# labeling user_id and session_id\n",
        "user_id_array = df_data['user_id'].unique()\n",
        "user_id_map = {val:idx for idx,val in enumerate(user_id_array)}\n",
        "df_data['user_id_new'] = df_data['user_id'].map(lambda x: user_id_map[x])\n",
        "session_id_array = df_data['session_id'].unique()\n",
        "session_id_map = {val:idx for idx,val in enumerate(session_id_array)}\n",
        "df_data['session_id_new'] = df_data['session_id'].map(lambda x: session_id_map[x])\n",
        "\n",
        "# labeling + one-hot for event_type\n",
        "event_list = ['', 'RELEASE', 'MOVE', 'WHEEL', 'DRAG', 'CLICK']\n",
        "df_data['e'] = df_data['event_type'].map(lambda x: event_list[x])\n",
        "one_hot_df = pd.get_dummies(df_data['e'], prefix='event')\n",
        "df_data = pd.concat([df_data, one_hot_df], axis=1)\n",
        "categorial_cols = ['event_MOVE', 'event_WHEEL', 'event_DRAG', 'event_CLICK']\n",
        "\n",
        "# time features\n",
        "df_data['datetime'] = pd.to_datetime(df_data['timestamp'], unit='ms')\n",
        "df_data['day'] = df_data['datetime'].dt.date\n",
        "df_data['hour'] = df_data['datetime'].dt.hour\n",
        "df_data['minute'] = df_data['datetime'].dt.minute\n",
        "df_data['second'] = df_data['datetime'].dt.second\n",
        "time_features = ['hour', 'minute', 'second']\n",
        "\n",
        "# timestamp features\n",
        "df_data['time_stamp_min'] = df_data.groupby('session_id_new')['timestamp'].transform('min')\n",
        "df_data['time_stamp'] = df_data['timestamp'] - df_data['time_stamp_min']\n",
        "df_data['time_diff'] = df_data.groupby('session_id_new')['timestamp'].diff()\n",
        "\n",
        "# calculate the x_diff y_diff and distance btw (x,y)\n",
        "df_data['x_diff'] = df_data.groupby('session_id_new')['screen_x'].diff()\n",
        "df_data['y_diff'] = df_data.groupby('session_id_new')['screen_y'].diff()\n",
        "df_data['xy_diff'] = np.sqrt(df_data['x_diff']**2 + df_data['y_diff']**2)\n",
        "\n",
        "# calculate the speed\n",
        "df_data['x_speed'] = np.abs(df_data['x_diff'] / df_data['time_diff'])\n",
        "df_data['y_speed'] = np.abs(df_data['y_diff'] / df_data['time_diff'])\n",
        "df_data['xy_speed'] = df_data['xy_diff'] / df_data['time_diff']\n",
        "\n",
        "mean_features = ['x_speed', 'y_speed', 'xy_speed', 'screen_x', 'screen_y', 'event_type']\n",
        "df_data = get_mean_features_sec(df_data.dropna(), mean_features, ['session_id_new', 'day', 'hour', 'minute', 'second'])\n",
        "\n",
        "# we take 30 step\n",
        "aggragation_cols = [ 'mean_second_x_speed', 'mean_second_y_speed', 'mean_second_xy_speed', \n",
        "                     'mean_second_screen_x', 'mean_second_screen_y', 'mean_second_event_type'\n",
        "                  ]\n",
        "n_step = 25\n",
        "df_data, features_add = get_shift(n_step, aggragation_cols, df_data, 'session_id_new')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y_wkcoc-gUY",
        "outputId": "e2e57f32-3736-4d25-ce48-52cbb4783f33"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 323 ms, sys: 4.96 ms, total: 328 ms\n",
            "Wall time: 329 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORdFlQsR-rfV",
        "outputId": "8ed347f9-f4f9-4942-a8c8-768f24ec48f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['index', 'uid', 'session_id', 'user_id', 'timestamp', 'event_type',\n",
              "       'screen_x', 'screen_y', 'user_id_new', 'session_id_new',\n",
              "       ...\n",
              "       'mean_second_xy_speed_shifted_24', 'mean_second_screen_x_shifted_24',\n",
              "       'mean_second_screen_y_shifted_24', 'mean_second_event_type_shifted_24',\n",
              "       'mean_second_x_speed_shifted_25', 'mean_second_y_speed_shifted_25',\n",
              "       'mean_second_xy_speed_shifted_25', 'mean_second_screen_x_shifted_25',\n",
              "       'mean_second_screen_y_shifted_25', 'mean_second_event_type_shifted_25'],\n",
              "      dtype='object', length=186)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Split"
      ],
      "metadata": {
        "id": "aTeAQLf1CkfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get test dataset: each user one session id\n",
        "def get_valid_idx(df_data_, n, random_seed = False):\n",
        "  if random_seed:\n",
        "    random.seed()\n",
        "  idx_valid = []\n",
        "  for i in df_data_.groupby('user_id_new')['session_id_new'].unique():\n",
        "    idx_valid.append(random.sample(list(i), 2)[0])\n",
        "    idx_valid.append(random.sample(list(i), 2)[1])\n",
        "  return np.sort(idx_valid)"
      ],
      "metadata": {
        "id": "a6tRu2whCj9F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_valid = get_valid_idx(df_data, 5)\n",
        "print(f\"The session_id picked as test dataset: \\n {idx_valid}\")\n",
        "\n",
        "df_valid = df_data[df_data['session_id_new'].isin(idx_valid)]; \n",
        "df_train = df_data[~df_data.index.isin(df_valid.index)]; \n",
        "df_valid = df_valid.dropna().reset_index()\n",
        "df_train = df_train.dropna().reset_index()\n",
        "\n",
        "df_train.shape, df_valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcHmqE-0ComQ",
        "outputId": "e8a56fd1-5912-4c21-863b-85004a14da49"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The session_id picked as test dataset: \n",
            " [  0   3   5   6  10  11  13  15  17  18  21  22  24  27  28  29  30  34\n",
            "  40  41  42  44  48  52  55  66  68  71  72  73  73  76  85  89  94  95\n",
            " 101 108 112 115]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5606, 187), (2907, 187))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_for_train =  features_add + aggragation_cols # numerical_cols + categorial_cols\n",
        "\n",
        "# get the label as well, check the data shape \n",
        "df_train_x = df_train[features_for_train]; df_train_y = df_train['user_id_new'] \n",
        "df_valid_x = df_valid[features_for_train];  df_valid_y = df_valid['user_id_new'] \n",
        "# when need test dataset: \n",
        "# df_test_x = df_test[features_for_train];  df_test_y = df_test['user_id_new'] \n",
        "\n",
        "df_train_x.shape, df_valid_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vaa56M-1DBXT",
        "outputId": "e9b90442-cb40-46ea-95c0-93c136b5f2c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5606, 156), (2907, 156))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RF"
      ],
      "metadata": {
        "id": "ymLHuLL9DOL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rf = RandomForestClassifier(n_estimators=1500, random_state=42)\n",
        "rf.fit(df_train_x.values, df_train_y.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWuwnp5IDMUA",
        "outputId": "f935836d-7d9d-48ef-fd99-25622c86f37d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 13s, sys: 588 ms, total: 1min 14s\n",
            "Wall time: 1min 15s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=1500, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(f\"confusion matrix on train dataset: \\n {classification_report(df_train_y.values, rf.predict(df_train_x))}\")\n",
        "print(f\"confusion matrix on train dataset: \\n {classification_report(df_valid_y.values, rf.predict(df_valid_x))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar7DaGs29_cr",
        "outputId": "468f102c-da51-4dac-8c7d-629cbdbeacee"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix on train dataset: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       164\n",
            "           1       1.00      1.00      1.00       242\n",
            "           2       1.00      1.00      1.00       319\n",
            "           3       1.00      1.00      1.00       318\n",
            "           4       1.00      1.00      1.00       223\n",
            "           5       1.00      1.00      1.00       262\n",
            "           6       1.00      1.00      1.00       367\n",
            "           7       1.00      1.00      1.00       357\n",
            "           8       1.00      1.00      1.00       369\n",
            "           9       1.00      1.00      1.00       304\n",
            "          10       1.00      1.00      1.00       202\n",
            "          11       1.00      1.00      1.00       239\n",
            "          12       1.00      1.00      1.00       318\n",
            "          13       1.00      1.00      1.00       306\n",
            "          14       1.00      1.00      1.00       187\n",
            "          15       1.00      1.00      1.00       412\n",
            "          16       1.00      1.00      1.00       294\n",
            "          17       1.00      1.00      1.00       199\n",
            "          18       1.00      1.00      1.00       291\n",
            "          19       1.00      1.00      1.00       233\n",
            "\n",
            "    accuracy                           1.00      5606\n",
            "   macro avg       1.00      1.00      1.00      5606\n",
            "weighted avg       1.00      1.00      1.00      5606\n",
            "\n",
            "confusion matrix on train dataset: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.20      0.24       106\n",
            "           1       0.28      0.24      0.26       111\n",
            "           2       0.37      0.32      0.34       146\n",
            "           3       0.36      0.79      0.49       152\n",
            "           4       0.91      0.67      0.77       204\n",
            "           5       0.82      0.71      0.76       164\n",
            "           6       0.43      0.27      0.33       247\n",
            "           7       0.38      0.05      0.08       235\n",
            "           8       0.53      0.66      0.59       152\n",
            "           9       0.03      0.01      0.02        94\n",
            "          10       0.49      0.30      0.37        77\n",
            "          11       0.27      0.43      0.34       138\n",
            "          12       0.62      0.56      0.59       113\n",
            "          13       0.27      0.51      0.35       109\n",
            "          14       0.60      0.74      0.66       151\n",
            "          15       0.52      0.56      0.54       184\n",
            "          16       0.90      0.91      0.91       200\n",
            "          17       0.18      0.10      0.13        96\n",
            "          18       0.08      0.31      0.12        71\n",
            "          19       0.11      0.06      0.08       157\n",
            "\n",
            "    accuracy                           0.44      2907\n",
            "   macro avg       0.42      0.42      0.40      2907\n",
            "weighted avg       0.46      0.44      0.43      2907\n",
            "\n"
          ]
        }
      ]
    }
  ]
}